{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Codigo completa el proceso de ETL luego del EDA'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Codigo completa el proceso de ETL luego del EDA\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir la ruta base de los archivos\n",
    "# Asegúrate de cambiar esta ruta si el dataset está en otra ubicación\n",
    "RUTA_BASE = \"C:/Users/oscar/Desktop/P1-HENRY/data_set\"\n",
    "#  Definir la ruta del entorno virtual para descargar NLTK\n",
    "NLTK_PATH = \"C:/Users/oscar/Desktop/P1-HENRY/env/nltk_data\"\n",
    "nltk.data.path.append(NLTK_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar el dataset final en formato Parquet\n",
    "ruta_final_parquet = os.path.join(RUTA_BASE, \"movies_final.parquet\")\n",
    "movies_df = pd.read_parquet(ruta_final_parquet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrar solo películas a partir de 1975\n",
    "movies_df = movies_df[movies_df['release_year'] >= 1975]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversión de tipos de datos\n",
    "columns_to_numeric = ['popularity', 'vote_count', 'budget', 'revenue']\n",
    "for col in columns_to_numeric:\n",
    "    movies_df[col] = pd.to_numeric(movies_df[col], errors='coerce')\n",
    "\n",
    "movies_df[['budget', 'revenue']] = movies_df[['budget', 'revenue']].fillna(0)\n",
    "movies_df['vote_average'] = movies_df['vote_average'].astype(float)\n",
    "movies_df['release_year'] = movies_df['release_year'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrar solo películas en los idiomas principales\n",
    "idiomas_principales = ['en', 'es', 'fr', 'it', 'de']\n",
    "movies_df = movies_df[movies_df['original_language'].isin(idiomas_principales)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminar columnas innecesarias para optimización\n",
    "columns_to_drop = ['return', 'budget', 'log_revenue', 'log_budget', \n",
    "                   'log_popularity', 'log_vote_count', 'production_countries_id', 'btc_id']\n",
    "movies_df = movies_df.drop(columns=columns_to_drop, errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:/Users/oscar/Desktop/P1-HENRY/env/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:/Users/oscar/Desktop/P1-HENRY/env/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:/Users/oscar/Desktop/P1-HENRY/env/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:/Users/oscar/Desktop/P1-HENRY/env/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:/Users/oscar/Desktop/P1-HENRY/env/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocesamiento del texto (descripción de la película)\n",
    "nltk.download('punkt', download_dir=NLTK_PATH)\n",
    "nltk.download('stopwords', download_dir=NLTK_PATH)\n",
    "nltk.download('wordnet', download_dir=NLTK_PATH)\n",
    "nltk.download('omw-1.4', download_dir=NLTK_PATH)\n",
    "nltk.download('averaged_perceptron_tagger', download_dir=NLTK_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"Limpia y normaliza el texto eliminando puntuaciones, stopwords y aplicando lematización.\"\"\"\n",
    "    if pd.isna(text) or not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = text.lower()\n",
    "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    tokens = word_tokenize(text)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    return \" \".join(tokens)\n",
    "# Aplicar preprocesamiento en la columna 'overview'\n",
    "movies_df['overview'] = movies_df['overview'].fillna(\"No description available.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear columna de predicción combinando diferentes atributos\n",
    "movies_df['predictor'] = (\n",
    "    movies_df['overview'] + \" \" +  # Descripción de la película\n",
    "    movies_df['genres'].fillna('') + \" \" +  # Géneros\n",
    "    movies_df['actors'].fillna('') + \" \" +  # Actores principales\n",
    "    movies_df['director'].fillna('') + # Director\n",
    "    movies_df['vote_average'].astype(str) + \" \" +\n",
    "    movies_df['popularity'].astype(str)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Datos procesados y guardados correctamente en: C:/Users/oscar/Desktop/P1-HENRY/data_set\\movies_dataset_processed.parquet\n"
     ]
    }
   ],
   "source": [
    "#  Exportar datos procesados en formato Parquet\n",
    "ruta_salida_parquet = os.path.join(RUTA_BASE, \"movies_dataset_processed.parquet\")\n",
    "movies_df.to_parquet(ruta_salida_parquet, engine='pyarrow', compression='snappy', index=False)\n",
    "\n",
    "print(\"✅ Datos procesados y guardados correctamente en:\", ruta_salida_parquet)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
